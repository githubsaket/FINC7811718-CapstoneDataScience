{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Domain based feature selection predict stock direction All -2018 .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxHymRJ_qWCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674ab285-79b5-4ee0-d0ac-d6c622c58b27"
      },
      "source": [
        "# Library loads\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from numpy import random\n",
        "from sklearn import preprocessing\n",
        "from sklearn import tree\n",
        "from sklearn import ensemble\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import Normalizer\n",
        "#from sklearn.preprocessing import Imputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score, mean_squared_error,mean_absolute_error, roc_curve, classification_report, auc)\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For creating plots using data \n",
        "'''\n",
        "import seaborn as sns\n",
        "import plotly\n",
        "from plotly import __version__\n",
        "print(plotly.__version__)\n",
        "import cufflinks as cf\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "init_notebook_mode(connected = True)\n",
        "cf.go_offline()\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = 'colab'\n",
        "'''\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA7DKF8vNtFu",
        "outputId": "5f315880-b409-4ff2-c65f-9cc1f05261b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6EwaDP-NHtC"
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1)\n",
        "\n",
        "seed=1\n",
        "pd.options.display.float_format = '{:, .0f}'.format\n",
        "data_2014 = pd.read_csv('/content/drive/My Drive/Data/2014_Financial_Data.csv', header=0)\n",
        "data_2015 = pd.read_csv('/content/drive/My Drive/Data/2015_Financial_Data.csv', header=0)\n",
        "data_2016 = pd.read_csv('/content/drive/My Drive/Data/2016_Financial_Data.csv', header=0)\n",
        "data_2017 = pd.read_csv('/content/drive/My Drive/Data/2017_Financial_Data.csv', header=0)\n",
        "data_2018 = pd.read_csv('/content/drive/My Drive/Data/2018_Financial_Data.csv', header=0)\n",
        "#print(data_2014.shape, data_2015.shape, data_2016.shape, data_2017.shape)\n",
        "#filename='/content/drive/My Drive/Data/2018_Financial_Data.csv'\n",
        "#print(filename)\n",
        "\n",
        "#data=pd.read_csv(filename, header=0)\n",
        "#data=pd.concat([data_2014,data_2015,data_2016,data_2017])\n",
        "\n",
        "#data = pd.DataFrame(data)\n",
        "#data_2018 =pd.DataFrame(data_2018)\n",
        "#data.shape, data_2018.shape\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyOoXwrjorqw"
      },
      "source": [
        "**Step 2 Create a function to identify only those variable as from the dataset that are relevant to target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oCB5qmCojAw"
      },
      "source": [
        "def transform(t):\n",
        "  t.rename(columns={'Unnamed: 0':'Company Name'},inplace=True)\n",
        "  t.replace(np.nan, 0)\n",
        "  t.drop(t[t['Total assets'] ==0].index, inplace = True) \n",
        "  t.drop(t[t['Revenue'] ==0].index, inplace = True) \n",
        "  t['Total Asset Turnover Ratio'] = t['Revenue'] / t['Total assets']\n",
        "  t['Gross Margin Ratio'] = t['Gross Margin'] / t['Revenue']\n",
        "  t['Net Profit Ratio'] = t['Net Profit Margin']/t['Revenue']\n",
        "  t['Return on Total Assets Ratio'] = t['Net Income']/t['Total assets']\n",
        "  features = ['Company Name', 'currentRatio','quickRatio', 'Free Cash Flow margin','debtRatio', 'debtEquityRatio',\n",
        "              'cashFlowToDebtRatio','Return on Tangible Assets','Total Asset Turnover Ratio',\n",
        "              'Return on Total Assets Ratio','ROE','Gross Margin Ratio',\n",
        "              'Net Profit Ratio', 'EPS','EPS Diluted','PE ratio', 'PB ratio','priceEarningsToGrowthRatio', 'Sector','Class']\n",
        "  t = t[features]\n",
        "\n",
        "  return t"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLUOEQmMoxin"
      },
      "source": [
        "**Applying the transformation to wrangle the raw dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mki0qIrOov4v"
      },
      "source": [
        "data2014 = transform(data_2014)\n",
        "#data2014['2015StockPriceVar'] = data_2014['2015 PRICE VAR [%]']\n",
        "data2015 = transform(data_2015)\n",
        "#data2015['2016StockPriceVar'] = data_2015['2016 PRICE VAR [%]']\n",
        "data2016 = transform(data_2016)\n",
        "#data_2016['2017StockPriceVar'] = data_2016['2017 PRICE VAR [%]']\n",
        "data2017 = transform(data_2017)\n",
        "#data2017['2018StockPriceVar'] = data_2017['2018 PRICE VAR [%]']\n",
        "data2018 = transform(data_2018)\n",
        "#data2018['2019StockPriceVar'] = data_2018['2019 PRICE VAR [%]']\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvBygtuYpzfI"
      },
      "source": [
        "Concatenate the four years data between 2014 until 2017 and separately create a data frame to load 2018 data for testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qvu6Yx8hp6Q-",
        "outputId": "3e813849-7d46-4a42-8af5-0e834b001c94"
      },
      "source": [
        "data = pd.concat([data2014,data2015,data2016,data2017])\n",
        "data = pd.DataFrame(data)\n",
        "data_2018 =pd.DataFrame(data2018)\n",
        "data.shape, data_2018.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16921, 20), (4194, 20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwJHq9TT3I7L"
      },
      "source": [
        "# Save the domain based raw data set\r\n",
        "\r\n",
        "## save to xlsx file\r\n",
        "\r\n",
        "filepath = '/content/drive/My Drive/Data/Result/domain_based.xlsx'\r\n",
        "\r\n",
        "data.to_excel(filepath, index=False)\r\n",
        "\r\n",
        "#domain-based= np.sav('/content/drive/My Drive/Data/Result/predictedRFbase.txt', y_pred, fmt='%01d')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_qXN3Nk7m_F",
        "outputId": "3429eba5-e58e-4fdb-9072-e8d976de281b"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 16921 entries, 0 to 4959\n",
            "Data columns (total 20 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   Company Name                  16921 non-null  object \n",
            " 1   currentRatio                  14156 non-null  float64\n",
            " 2   quickRatio                    14159 non-null  float64\n",
            " 3   Free Cash Flow margin         14931 non-null  float64\n",
            " 4   debtRatio                     14908 non-null  float64\n",
            " 5   debtEquityRatio               14908 non-null  float64\n",
            " 6   cashFlowToDebtRatio           11995 non-null  float64\n",
            " 7   Return on Tangible Assets     11558 non-null  float64\n",
            " 8   Total Asset Turnover Ratio    15478 non-null  float64\n",
            " 9   Return on Total Assets Ratio  15496 non-null  float64\n",
            " 10  ROE                           14887 non-null  float64\n",
            " 11  Gross Margin Ratio            15799 non-null  float64\n",
            " 12  Net Profit Ratio              15411 non-null  float64\n",
            " 13  EPS                           15706 non-null  float64\n",
            " 14  EPS Diluted                   15713 non-null  float64\n",
            " 15  PE ratio                      14962 non-null  float64\n",
            " 16  PB ratio                      12948 non-null  float64\n",
            " 17  priceEarningsToGrowthRatio    10235 non-null  float64\n",
            " 18  Sector                        16921 non-null  object \n",
            " 19  Class                         16921 non-null  int64  \n",
            "dtypes: float64(17), int64(1), object(2)\n",
            "memory usage: 2.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2XrqynvP0uD"
      },
      "source": [
        "**Creating function for Evaluation using Accuracy, Precision**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzLlwqlKPBtv",
        "outputId": "c542cf84-1e8f-4c83-b450-fd92a55ad651"
      },
      "source": [
        "print(data.iloc[:, -1].unique().size)\n",
        "\n",
        "if data.iloc[:, -1].unique().size==2:\n",
        "    def evaluate(y_test, y_pred, y_scores):\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='binary', pos_label=1) #labels=np.unique(y_pred))\n",
        "        recall = recall_score(y_test, y_pred, average='binary', pos_label=1) #labels=np.unique(y_pred))\n",
        "        f1 = f1_score(y_test, y_pred, average='binary', pos_label=1) #labels=np.unique(y_pred))\n",
        "        auc = roc_auc_score(y_test, y_scores)\n",
        "        return [accuracy, precision, recall, f1, auc]\n",
        "else:\n",
        "\n",
        "    def evaluate(y_test, y_pred, y_scores):\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted', pos_label=1) #labels=np.unique(y_pred))\n",
        "        recall = recall_score(y_test, y_pred, average='weighted', pos_label=1) #labels=np.unique(y_pred))\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted', pos_label=1) #labels=np.unique(y_pred))\n",
        "\t\t\t\t#metrics.f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))\t\t \n",
        "\n",
        "        auc = roc_auc_score(y_test, y_scores)\n",
        "        return [accuracy, precision, recall, f1, auc]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In2V7VkTBhjl"
      },
      "source": [
        "Imputing zeros where there are NaNs. and dropping all price variance columns from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW4QXzL7Urd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "159997af-6490-4a89-c831-3326b0388516"
      },
      "source": [
        "data = data.drop(['2015 PRICE VAR [%]','2016 PRICE VAR [%]','2017 PRICE VAR [%]','2018 PRICE VAR [%]'], axis=1)\n",
        "data_2018 = data_2018.drop(['2019StockPriceVar'], axis=1)\n",
        "data.fillna(0, inplace=True)\n",
        "data_2018.fillna(0, inplace=True)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-771329dc16d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2015 PRICE VAR [%]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2016 PRICE VAR [%]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2017 PRICE VAR [%]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2018 PRICE VAR [%]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_2018\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_2018\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2019StockPriceVar'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_2018\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4172\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m         )\n\u001b[1;32m   4176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3888\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3889\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3921\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3923\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3924\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5287\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5288\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['2015 PRICE VAR [%]' '2016 PRICE VAR [%]' '2017 PRICE VAR [%]'\\n '2018 PRICE VAR [%]'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVJ6sheZawY9",
        "outputId": "aa47a9a8-4e0b-49a3-e4f3-893ca0ecd082"
      },
      "source": [
        "data.info()\n",
        "data_2018.info()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 16921 entries, 0 to 4959\n",
            "Data columns (total 20 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   Company Name                  16921 non-null  object \n",
            " 1   currentRatio                  14156 non-null  float64\n",
            " 2   quickRatio                    14159 non-null  float64\n",
            " 3   Free Cash Flow margin         14931 non-null  float64\n",
            " 4   debtRatio                     14908 non-null  float64\n",
            " 5   debtEquityRatio               14908 non-null  float64\n",
            " 6   cashFlowToDebtRatio           11995 non-null  float64\n",
            " 7   Return on Tangible Assets     11558 non-null  float64\n",
            " 8   Total Asset Turnover Ratio    15478 non-null  float64\n",
            " 9   Return on Total Assets Ratio  15496 non-null  float64\n",
            " 10  ROE                           14887 non-null  float64\n",
            " 11  Gross Margin Ratio            15799 non-null  float64\n",
            " 12  Net Profit Ratio              15411 non-null  float64\n",
            " 13  EPS                           15706 non-null  float64\n",
            " 14  EPS Diluted                   15713 non-null  float64\n",
            " 15  PE ratio                      14962 non-null  float64\n",
            " 16  PB ratio                      12948 non-null  float64\n",
            " 17  priceEarningsToGrowthRatio    10235 non-null  float64\n",
            " 18  Sector                        16921 non-null  object \n",
            " 19  Class                         16921 non-null  int64  \n",
            "dtypes: float64(17), int64(1), object(2)\n",
            "memory usage: 2.7+ MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4194 entries, 0 to 4391\n",
            "Data columns (total 20 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   Company Name                  4194 non-null   object \n",
            " 1   currentRatio                  3951 non-null   float64\n",
            " 2   quickRatio                    3952 non-null   float64\n",
            " 3   Free Cash Flow margin         3945 non-null   float64\n",
            " 4   debtRatio                     3948 non-null   float64\n",
            " 5   debtEquityRatio               3948 non-null   float64\n",
            " 6   cashFlowToDebtRatio           3430 non-null   float64\n",
            " 7   Return on Tangible Assets     3096 non-null   float64\n",
            " 8   Total Asset Turnover Ratio    4044 non-null   float64\n",
            " 9   Return on Total Assets Ratio  4044 non-null   float64\n",
            " 10  ROE                           3943 non-null   float64\n",
            " 11  Gross Margin Ratio            4147 non-null   float64\n",
            " 12  Net Profit Ratio              4022 non-null   float64\n",
            " 13  EPS                           4130 non-null   float64\n",
            " 14  EPS Diluted                   4132 non-null   float64\n",
            " 15  PE ratio                      3947 non-null   float64\n",
            " 16  PB ratio                      3404 non-null   float64\n",
            " 17  priceEarningsToGrowthRatio    2712 non-null   float64\n",
            " 18  Sector                        4194 non-null   object \n",
            " 19  Class                         4194 non-null   int64  \n",
            "dtypes: float64(17), int64(1), object(2)\n",
            "memory usage: 688.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boZVP67kQNtp"
      },
      "source": [
        "**Replace all NaN with 0 and blanks with 0 Enable coding to convert Sector**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYSnth1mPznL",
        "outputId": "18c34e6d-3841-41fc-92a4-d4aa37d11d56"
      },
      "source": [
        "#data.fillna(0, inplace=True)\n",
        "#data_2016.fillna(0, inplace=True)\n",
        "\n",
        "# Categorical boolean mask\n",
        "categorical_feature_mask = data.dtypes==object\n",
        "# filter categorical columns using mask and turn it into a list\n",
        "categorical_cols = data.columns[categorical_feature_mask].tolist()\n",
        "#categorical_cols1 = data_2018.columns[categorical_feature_mask].tolist()\n",
        "\n",
        "print(categorical_cols)\n",
        "#Use LabelEncoder() to transfer categorical data to numurical data\n",
        "le=LabelEncoder()\n",
        "print(len(categorical_cols))\n",
        "if len(categorical_cols)!=0:\n",
        "    data[categorical_cols] = data[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
        "    #data_2018[categorical_cols] = data_2018[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
        "#data[categorical_cols].head(5)\n",
        "#data_2018[categorical_cols1].head(5)\n",
        "\n",
        "data=np.array(data)\n",
        "data_2018=np.array(data_2018)\n",
        "#If first column is ID, then manuly removed from the dataset\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Company Name', 'Sector']\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fEEAsSnbhs9"
      },
      "source": [
        "#identify target baseline\n",
        "#target_class = 'Class'\n",
        "#data_target_class = data[target_class].mode()\n",
        "#data_2018_target_class =data_2018[target_class].mode()\n",
        "#data_target_class, data_2018_target_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A05cfnffPBZm"
      },
      "source": [
        "**Define Train and Test data and Label data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqsD2J52QpzV"
      },
      "source": [
        "#X,y=data[:,1:-4], data[:, -1]\n",
        "#X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.20, random_state=seed)\n",
        "#y_train=y_train.astype(int)\n",
        "\n",
        "X_train,y_train=data[:,1:-3], data[:, -1]\n",
        "X_test,y_test=data_2018[:,1:-3], data_2018[:, -1]\n",
        "#ros = RandomOverSampler(random_state=0)\n",
        "\n",
        "#X_train, y_train = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "X_train2 = scaler.transform(X_train)\n",
        "X_test2 = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "traindata = np.array(X_train2)\n",
        "trainlabel = np.array(y_train)\n",
        "\n",
        "testdata = np.array(X_test2)\n",
        "testlabel = np.array(y_test)\n",
        "\n",
        "traindata=traindata.astype(float)\n",
        "trainlabel=trainlabel.astype(int)\n",
        "testdata=testdata.astype(float)\n",
        "testlabel = testlabel.astype(int)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTQSd3s-T8y9"
      },
      "source": [
        "**MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkHjU8z6T9om"
      },
      "source": [
        "def MLP(X_train,y_train,X_test,y_test):\n",
        "    clf = MLPClassifier(solver='lbfgs', activation= 'relu', alpha=1e-5,hidden_layer_sizes=(10, 5), random_state=seed)\n",
        "    \n",
        "    l=X_train[0].size\n",
        "    lr = np.logspace(-5,-1, 5)\n",
        "    #print(lr)\n",
        "    #print(X_train[0].size)\n",
        "    hz=[(10, 5),(10,5,3)]\n",
        "    param_grid = {'alpha': lr,'hidden_layer_sizes':hz}\n",
        "\n",
        "    gridcv = sklearn.model_selection.GridSearchCV(clf, param_grid, verbose=1, cv=3)\n",
        "    gridcv.fit(X_train, y_train)\n",
        "\t\t\t\t\t \n",
        "\n",
        "    gridcv = gridcv.fit(X_train,y_train)\n",
        "    y_pred = gridcv.best_estimator_.predict(X_test)\n",
        "    y_scores = gridcv.best_estimator_.predict_proba(X_test)[:,1]\n",
        "\n",
        "    print(\"best parameters:\", gridcv.best_params_)\n",
        "    np.savetxt('/content/drive/My Drive/Data/Result/predictedMLP.txt', y_pred, fmt='%01d')\n",
        "    return evaluate(y_test,y_pred,y_scores)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duLuPzLV2F3o"
      },
      "source": [
        "# Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWRkhtH7ULzV"
      },
      "source": [
        "def LogisticRegression(X_train,y_train,X_test,y_test):\n",
        "    clf = sklearn.linear_model.LogisticRegression(random_state=seed, solver='lbfgs')\n",
        "    clf = clf.fit(X_train,y_train)\n",
        "\t\t\t\t\t\t\t\t   \n",
        "\t\t\t\t\t \n",
        "\t\t\t\t\t\t\t \n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_scores = clf.predict_proba(X_test)[:,1]\n",
        "    np.savetxt('/content/drive/My Drive/Data/Result/predictedLR.txt', y_pred, fmt='%01d')\n",
        "    return evaluate(y_test,y_pred,y_scores)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uvGKFbTUSPV"
      },
      "source": [
        "**Running the Decision tree model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgdXwf9EUR41"
      },
      "source": [
        "def CART(X_train,y_train,X_test,y_test):\n",
        "   # clf = tree.DecisionTreeClassifier(max_depth=5)\n",
        "   # clf = tree.DecisionTreeClassifier(criterion='entropy',max_depth=5,min_samples_leaf=5)\n",
        "\n",
        "    clf = tree.DecisionTreeClassifier( random_state=seed)\n",
        "\n",
        "    md=np.arange(1,50,10)\n",
        "    param_grid = {'max_depth': md}\n",
        "\t\t\t\t\t \n",
        "\t\t\t\t\t\t\t \n",
        "\n",
        "    gridcv = sklearn.model_selection.GridSearchCV(clf, param_grid, verbose=1, cv=3)\n",
        "    gridcv.fit(X_train, y_train)\n",
        "\n",
        "    #clf = clf.fit(X_train,y_train)\n",
        "    y_pred = gridcv.best_estimator_.predict(X_test)\n",
        "    y_scores = gridcv.best_estimator_.predict_proba(X_test)[:,1]\n",
        "    np.savetxt('/content/drive/My Drive/Data/Result/predictedDTbase.txt', y_pred, fmt='%01d')\n",
        "    print(\"best parameters:\", gridcv.best_params_)\n",
        "    #tree.plot_tree(clf.fit(X_train,y_train))\n",
        "    #tree.export_graphviz()\n",
        "    #dot_data = tree.export_graphviz(clf, out_file=None)\n",
        "    #graph = graphviz.Source(dot_data)\n",
        "    #graph.render(\"tree\")\n",
        "    return evaluate(y_test,y_pred,y_scores)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2tFYWqVURrt"
      },
      "source": [
        "# Random Forest model*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_T5G3TBUot0"
      },
      "source": [
        "#randomforest\n",
        "\n",
        "def RandomForest(X_train,y_train,X_test,y_test):\n",
        "    clf = ensemble.RandomForestClassifier(max_features=None,random_state=seed)\n",
        "\n",
        "    md = np.arange(1, 50, 10)\n",
        "    ne = np.arange(1, 50, 10)\n",
        "    param_grid = {'max_depth': md,'n_estimators':ne}\n",
        "\t\t\t\t\t \n",
        "\t\t\t\t\t\t\t \n",
        "\n",
        "    gridcv = sklearn.model_selection.GridSearchCV(clf, param_grid, verbose=1, cv=2)\n",
        "    #gridcv = sklearn.model_selection.GridSearchCV(clf, param_grid, verbose=1, cv=3)\n",
        "    gridcv.fit(X_train, y_train)\n",
        "\n",
        "    gridcv = gridcv.fit(X_train,y_train)\n",
        "    y_pred = gridcv.best_estimator_.predict(X_test)\n",
        "    y_scores = gridcv.best_estimator_.predict_proba(X_test)[:,1]\n",
        "    print(\"best parameters:\", gridcv.best_params_)\n",
        "    np.savetxt('/content/drive/My Drive/Data/Result/predictedRFbase.txt', y_pred, fmt='%01d')\n",
        "    return evaluate(y_test,y_pred,y_scores)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0phyEqFDUtJk"
      },
      "source": [
        "**SVM Liner model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NJ_AZfRogET"
      },
      "source": [
        "def svm_one_class(X_train,y_train,X_test,y_test):\n",
        "    _train = preprocessing.normalize(X_train, norm='l2')\n",
        "    _test = preprocessing.normalize(X_test, norm='l2')\n",
        "    lin_clf = svm.LinearSVC()\n",
        "    lin_clf.fit(X_train, y_train)\n",
        "    LinearSVC(C=1000, class_weight=None, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "     multi_class='ovr', penalty='l2', random_state=seed, tol=0.0001,\n",
        "     verbose=0)\n",
        "    y_pred = lin_clf.predict(X_test)\n",
        "    y_scores = lin_clf.decision_function(X_test)\n",
        "    np.savetxt('/content/drive/My Drive/Data/Result/predictedSVMbase.txt', y_pred, fmt='%01d')\n",
        "    return evaluate(y_test,y_pred,y_scores)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4uX3_J1Ut4k"
      },
      "source": [
        "def SVM(X_train,y_train,X_test,y_test):\n",
        "\t\n",
        "    svm = sklearn.svm.SVC(kernel='rbf')\n",
        "    C_grid = np.logspace(0, 3, 4)\n",
        "    gamma_grid = np.logspace(-2, 1, 4)\n",
        "    param_grid = {'C': C_grid, 'gamma': gamma_grid}\n",
        "    gridcv = sklearn.model_selection.GridSearchCV(svm, param_grid, verbose=1, cv=2)\n",
        "    #gridcv = sklearn.model_selection.GridSearchCV(svm, param_grid, verbose=1, cv=3)\n",
        "    gridcv.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = gridcv.best_estimator_.predict(X_test)\n",
        "    y_scores = gridcv.best_estimator_.decision_function(X_test)\n",
        "    print(\"best parameters:\", gridcv.best_params_)\n",
        "    np.savetxt('/content/drive/My Drive/Data/Result/predictedSVMNormbase.txt', y_pred, fmt='%01d')\n",
        "    return evaluate(y_test,y_pred,y_scores)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vNYdJxjU-r0"
      },
      "source": [
        "Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smHrLrvaU-Jl"
      },
      "source": [
        "def Gaussian_NB(X_train,y_train,X_test,y_test):\n",
        "    gnb = GaussianNB()\n",
        "\t\t   \n",
        "    y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
        "\t\t\t\t\t\t\t \n",
        "\n",
        "\t\t\t\t\t\t\t\t\n",
        "    y_scores  = gnb.fit(X_train, y_train).predict_proba(X_test)[:,1]\n",
        "\n",
        "    np.savetxt('/content/drive/My Drive/Data/Result/predictedNBbase.txt', y_pred, fmt='%01d')\n",
        "    return evaluate(y_test,y_pred,y_scores)\n",
        "\n",
        " "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPMwfBxoL9ci"
      },
      "source": [
        "def kNN(X_train,y_train,X_test,y_test):\n",
        "    clf = KNeighborsClassifier()\n",
        "    #n_neighbors=5\n",
        "    nn=np.arange(1,100,10)\n",
        "    param_grid = {'n_neighbors': nn}\n",
        "\n",
        "    gridcv = sklearn.model_selection.GridSearchCV(clf, param_grid, verbose=1, cv=3)\n",
        "    gridcv.fit(X_train, y_train)\n",
        "    y_pred = gridcv.best_estimator_.predict(X_test)\n",
        "    y_scores = gridcv.best_estimator_.predict_proba(X_test)[:,1]\n",
        "    print(\"best parameters:\", gridcv.best_params_)\n",
        "    np.savetxt('/content/drive/My Drive/Data/Result/predictedKNNbase.txt', y_pred, fmt='%01d')\n",
        "    return evaluate(y_test,y_pred,y_scores)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFNJ1livVQpd"
      },
      "source": [
        "** XG Boost model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhfhXc1_VP9A"
      },
      "source": [
        "def AdaBoost(X_train,y_train,X_test,y_test):\n",
        "   # dt_stump = DecisionTreeClassifier(max_depth=5, min_samples_leaf=1)\n",
        "    dt_stump = DecisionTreeClassifier()\n",
        "    dt_stump.fit(X_train, y_train)\n",
        "    dt_stump_err = 1.0 - dt_stump.score(X_test, y_test)\n",
        "\n",
        "    clf = ensemble.AdaBoostClassifier(base_estimator=dt_stump,learning_rate=0.1, algorithm='SAMME')\n",
        "    clf= clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    #y_pred = clf.predict(X_test)\n",
        "    y_scores = clf.predict_proba(X_test)[:,1]\n",
        "    print(\"best parameters:\", gridcv.best_params_)\n",
        "    np.savetxt('/content/drive/My Drive/Data/Result/predictedABbase.txt', y_pred, fmt='%01d')\n",
        "\n",
        "    return evaluate(y_test,y_pred,y_scores)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_tg-y9LMdIh"
      },
      "source": [
        "def GBDT(X_train,y_train,X_test,y_test):\n",
        "    n_estimators = [10,50,100]\n",
        "    learning_rate = [0.01,0.1,1]\n",
        "    max_depth=[1,5,10]\n",
        "    param_grid = {'n_estimators': n_estimators, 'learning_rate': learning_rate,'max_depth':max_depth}\n",
        "\n",
        "    clf = ensemble.GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=seed)\n",
        "    #clf=clf.fit(X_train, y_train)\n",
        "\n",
        "    gridcv = sklearn.model_selection.GridSearchCV(clf, param_grid, verbose=1, cv=3)\n",
        "    gridcv.fit(X_train, y_train)\n",
        "    y_pred = gridcv.best_estimator_.predict(X_test)\n",
        "    y_scores = gridcv.best_estimator_.predict_proba(X_test)[:,1]\n",
        "    print(\"best parameters:\", gridcv.best_params_)\n",
        "    np.savetxt('/content/drive/My Drive/Data/Result/predictedBGDTbase.txt', y_pred, fmt='%01d')\n",
        "    return evaluate(y_test,y_pred,y_scores)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtCNwKgzVPlz"
      },
      "source": [
        "Print the model results for NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "SNE6sbMNnGjM",
        "outputId": "78f2a513-8bd2-44b6-9163-c91a33b69623"
      },
      "source": [
        "print(\"NB training\")\n",
        "a1= Gaussian_NB(traindata,trainlabel,testdata,testlabel)\n",
        "print(a1)\n",
        "#print(\"XG Boost training\")\n",
        "#a2=xgboost(traindata,trainlabel,testdata,testlabel)\n",
        "#print(a2)\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-2212778836cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NB training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mGaussian_NB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(\"XG Boost training\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#a2=xgboost(traindata,trainlabel,testdata,testlabel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-06f1d025114b>\u001b[0m in \u001b[0;36mGaussian_NB\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mgnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m--> 208\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "lJUBXhGMCuLn",
        "outputId": "0186b2bc-fb72-474a-f92c-25ac58694261"
      },
      "source": [
        "print(\"LR training\")\n",
        "a2=LogisticRegression(traindata,trainlabel,testdata,testlabel)\n",
        "print(a2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-95408aacccf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LR training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-209d4df6ae9d>\u001b[0m in \u001b[0;36mLogisticRegression\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1527\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8TvswtEnSDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0bf51b7-b9be-47a1-dd72-da81e1f1361c"
      },
      "source": [
        "print(\"MLP training\")\n",
        "a3=MLP(traindata,trainlabel,testdata,testlabel)\n",
        "print(a3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP training\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  1.5min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  1.5min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best parameters: {'alpha': 0.0001, 'hidden_layer_sizes': (10, 5)}\n",
            "[0.6090619307832422, 0.6440718999971404, 0.6090619307832422, 0.6211225741667199, 0.6261105105568017]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqQ3iN7vC4jD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f89891-d0d7-4c03-9c79-069a4e4d5b4b"
      },
      "source": [
        "print(\"RF training\")\n",
        "a5=RandomForest(traindata,trainlabel,testdata,testlabel)\n",
        "print(a5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RF training\n",
            "Fitting 2 folds for each of 25 candidates, totalling 50 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 18.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 25 candidates, totalling 50 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 18.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best parameters: {'max_depth': 41, 'n_estimators': 41}\n",
            "[0.61316029143898, 0.6717874078198804, 0.61316029143898, 0.6279215737407491, 0.656932849355938]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YooFWaArDAO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc10a80-0920-425b-c905-6f089928d970"
      },
      "source": [
        "print(\"DT training\")\n",
        "a6=CART(traindata,trainlabel,testdata,testlabel)\n",
        "print(a6)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DT training\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   35.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best parameters: {'max_depth': 11}\n",
            "[0.6038251366120219, 0.6563383014534346, 0.6038251366120219, 0.6184954958970235, 0.5933747179210501]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9geTZf-VSz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7346b7-8a7b-449b-99b6-06f63d9b88ca"
      },
      "source": [
        "print(\"SVM training\")\n",
        "a4= SVM(traindata,trainlabel,testdata,testlabel)\n",
        "print(a4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM training\n",
            "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed: 43.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "best parameters: {'C': 10.0, 'gamma': 1.0}\n",
            "[0.6063296903460837, 0.6069006245690755, 0.6063296903460837, 0.6066137595752733, 0.5581443619820503]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU-tThEKnesD"
      },
      "source": [
        "\n",
        "Result = ['a1','a2', 'a3','a4','a5','a6']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYIVQnkLneQ7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c181F-O4Xa_6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkBmi1fJXbnI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "ea434ff6-b5db-41ec-b18d-643e57477e61"
      },
      "source": [
        "print(\"Naive_bayes\")\n",
        "print(\"[accuracy,precision,recall,f1,auc]\")\n",
        "print('a1')\n",
        "\n",
        "print(\"NB training\")\n",
        "a2=Gaussian_NB(traindata,trainlabel,testdata,testlabel)\n",
        "print(a2)\n",
        "\n",
        "print(\"LogisticRegression\")\n",
        "print(\"[accuracy,precision,recall,f1,auc]\")\n",
        "print(a3)\n",
        "\n",
        "print(\"SVM\")\n",
        "print(\"[accuracy,precision,recall,f1,auc]\")\n",
        "print(a4)\n",
        "\n",
        "\n",
        "print(\"MLP\")\n",
        "print(\"[accuracy,precision,recall,f1,auc]\")\n",
        "print(a5)\n",
        "\n",
        "print(\"RandomForest\")\n",
        "print(\"[accuracy,precision,recall,f1,auc]\")\n",
        "print(a6)\n",
        "\n",
        "print(\"CART\")\n",
        "print(\"[accuracy,precision,recall,f1,auc]\")\n",
        "print(a7)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive_bayes\n",
            "[accuracy,precision,recall,f1,auc]\n",
            "a1\n",
            "NB training\n",
            "[0.6816939890710383, 0.581596705337981, 0.6816939890710383, 0.5805745574922684, 0.5026395662740407]\n",
            "LogisticRegression\n",
            "[accuracy,precision,recall,f1,auc]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-6ebf94f967df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LogisticRegression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[accuracy,precision,recall,f1,auc]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'a3' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsbtDWOJYIGQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfD3aDczYHLw"
      },
      "source": [
        "def tsneShow(X,y,Xt,yt):\n",
        "    yys = np.array(y)\n",
        "    yyt = np.array(yt)\n",
        "    for i in range(0, yyt.size):\n",
        "        if yyt[i] == 0:\n",
        "            yyt[i] = 2\n",
        "        elif yyt[i] == 1:\n",
        "            yyt[i] = 3\n",
        "\n",
        "    H = np.vstack((X, Xt))\n",
        "    Y = np.concatenate((yys, yyt), axis=0)\n",
        "\n",
        "    pca = PCA().fit_transform(H)\n",
        "    tsne = TSNE(n_components=2, learning_rate=100).fit_transform(H)\n",
        "\n",
        "    markers = ('.')\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # plt.subplot(121)\n",
        "    plt.scatter(tsne[:, 0], tsne[:, 1], c=Y, marker=markers[0], cmap=plt.cm.gist_rainbow)\n",
        "\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXh7cOoqYG4n"
      },
      "source": [
        ""
      ]
    }
  ]
}